import torch
import torchaudio
import pyaudio
import numpy as np
import torch.nn.functional as F
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification

# Initialize the model and feature extractor
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("superb/wav2vec2-large-superb-er")
model = Wav2Vec2ForSequenceClassification.from_pretrained("superb/wav2vec2-large-superb-er")
model.eval()

# Initialize PyAudio
p = pyaudio.PyAudio()

# Audio stream parameters
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000  # Sampling rate for Wav2Vec2 models
CHUNK = 1024  # Frame size

# Open audio stream
stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)

def predict_emotion(audio_chunk):
    # Convert audio chunk to numpy array
    audio_array = np.frombuffer(audio_chunk, dtype=np.int16).astype(np.float32) / 32768.0
    
    # Preprocess audio and make prediction
    inputs = feature_extractor(audio_array, sampling_rate=RATE, return_tensors="pt", padding=True)
    with torch.no_grad():
        logits = model(**inputs).logits
        scores = F.softmax(logits, dim=1).cpu().numpy()[0]
    
    # Get emotion labels and scores
    predicted_emotions = [{"Label": model.config.id2label[i], "Score": round(score * 100, 2)} for i, score in enumerate(scores)]
    return predicted_emotions

print("Recording and analyzing emotions in real-time... Press Ctrl+C to stop.")
try:
    while True:
        audio_chunk = stream.read(CHUNK)
        emotions = predict_emotion(audio_chunk)
        print("Emotions:", emotions)

except KeyboardInterrupt:
    print("Stopped.")
    stream.stop_stream()
    stream.close()
    p.terminate()
